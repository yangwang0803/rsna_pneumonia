{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d320f90f432c331afea02ef66fcddb59448ea200"
   },
   "source": [
    "# Base Notebook\n",
    "Aim to modularize and modulerize the workflow.\n",
    "All development should be done in this notebook.\n",
    "When training, fork one, configurate the config session only, run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import random\n",
    "import pydicom\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage import io\n",
    "from skimage import measure\n",
    "from skimage.transform import resize\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0982e76fee2f081daeab715ca463ff4b0c6aed6b"
   },
   "source": [
    "# Config Session\n",
    "All modules, including data prep, model, training modules, are going to be defined in abstract layer functions.\n",
    "Main body calls actual function according to indices specified here. \n",
    "\n",
    "*_func_idx is an index of function to be used in *_func_list\n",
    "*_param_idx is an index of paramters, in tuple, to be used in *_func_list[*_func_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2fce51e51427d29b0b1175e38e167552bf8d8da1"
   },
   "outputs": [],
   "source": [
    "#we may define model names here for easier reference, to be used in next block\n",
    "RESNET5 = 1\n",
    "UNET = 2\n",
    "\n",
    "#Data preparation options\n",
    "NO_MOD = 1\n",
    "\n",
    "#loss functions options\n",
    "BCE_IOU = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e145ca4294905f554e5cc8af34225f2433f347c5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#only change this cell after forking out, values are indices. Max values depends on number of functions defined. \n",
    "# 0 is example function. Do not use 0 for function in actual run.\n",
    "(data_func_idx, model_func_idx, loss_func_idx) = (NO_MOD,UNET,BCE_IOU)\n",
    "(data_param_idx, model_param_idx, loss_param_idx) = (0,0,0)\n",
    "EPOCHS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "24b7846cf45002127db94b3941a8a695364ab062"
   },
   "source": [
    "# Abstract Layer Definition\n",
    "Define list of function pointers, and paramters used by each function. Do not change this block below\n",
    "\n",
    "*_func_list is an 1 D array of function pointers\n",
    "\n",
    "*_param_list is a 2D array of tuples containing paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "078f2f1d742b7df45c2bbfd986239409eb3db001"
   },
   "outputs": [],
   "source": [
    "data_func_list = []\n",
    "model_func_list = []\n",
    "loss_func_list = []\n",
    "data_param_list = []\n",
    "model_param_list = []\n",
    "loss_param_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0d12d1399fc7799e5e97212578dc47e8bf0ecc5f"
   },
   "source": [
    "### Example for adding data preprocessing function\n",
    "This section shows a dummy example how a data preprocessing function and paramters are added to abstract layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b7b9b7d3bbd291487f5a9188e06d09a948cce7d1"
   },
   "outputs": [],
   "source": [
    "#Add data prep function\n",
    "def data_func0():\n",
    "    #load model_param\n",
    "    (param0,param1) = data_param_list[data_func_idx][data_param_idx]\n",
    "    print('data_func0: param0[%d], param1[%d]'%(param0,param1))\n",
    "\n",
    "#add data prep function and paramters \n",
    "i = len(data_func_list)                #find idx of next entry\n",
    "data_func_list.append(data_func0)     #append the function pointer to the list, replace with your function name\n",
    "data_param_list.append([])             #keep this line even if there's no param\n",
    "data_param_list[i].append((0,0))       #append as many parameter tuples as you want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2d19ad36d0349524807e0f3c5d46a100a48cefd4"
   },
   "source": [
    "### Example for adding model function\n",
    "This section shows a dummy example how a model function and paramters are added to abstract layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "07ce48a2f5d4cacd33008de2536bb2bb9c098f84"
   },
   "outputs": [],
   "source": [
    "#Add a new model\n",
    "def model_func0(input_size):\n",
    "    #load model_param\n",
    "    (param0,param1) = model_param_list[model_func_idx][model_param_idx]\n",
    "    print('model_func0: param0[%d], param1[%d]'%(param0,param1))\n",
    "    \n",
    "    #build model - dummy example\n",
    "    inputs = keras.Input(shape=(input_size, input_size, 1))\n",
    "    x = keras.layers.Conv2D(1, 1, activation='sigmoid')(inputs)\n",
    "    outputs = keras.layers.UpSampling2D(param0**param1)(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "#add model and paramters \n",
    "i = len(model_func_list)                #find idx of next entry\n",
    "model_func_list.append(model_func0)     #append the function pointer to the list, replace with your function name\n",
    "model_param_list.append([])             #keep this line even if there's no param\n",
    "model_param_list[i].append((0,0))       #append as many parameter tuples as you want\n",
    "model_param_list[i].append((0,1))\n",
    "model_param_list[i].append((0,2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4aa57c8046abad064a974a16d898ca8d451022e8"
   },
   "source": [
    "### Example for adding loss function\n",
    "This section shows a dummy example how a loss function and paramters are added to abstract layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "94587dcd96877606124966c89cd9867112e0b0ca"
   },
   "outputs": [],
   "source": [
    "#add a loss function\n",
    "def loss_func0(y_true, y_pred):\n",
    "    #load model_param\n",
    "    (param0,param1) = loss_param_list[loss_func_idx][loss_param_idx]\n",
    "    print('model_func1: param0[%d], param1[%d]'%(param0,param1))\n",
    "    \n",
    "#add loss function and paramters \n",
    "i = len(loss_func_list)                #find idx of next entry\n",
    "loss_func_list.append(loss_func0)      #append the function pointer to the list, replace with your function name\n",
    "loss_param_list.append([])             #keep this line even if there's no param\n",
    "loss_param_list[i].append((10,1))   #append as many parameter tuples as you want\n",
    "loss_param_list[i].append((10,2))\n",
    "loss_param_list[i].append((10,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "276b80f59fa9acdfd9307d74cee5f705cd6aa5b6",
    "collapsed": true
   },
   "source": [
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ded9e4cb9a1c019391bbbfb8893674ab19b5f045"
   },
   "source": [
    "# Build Data Preprocessing Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e255d1fce03c66b1ce919e1bb8f17a1c2b07dd77"
   },
   "source": [
    "## Load pneumonia locations\n",
    "\n",
    "Table contains [filename : pneumonia location] pairs per row. \n",
    "* If a filename contains multiple pneumonia, the table contains multiple rows with the same filename but different pneumonia locations. \n",
    "* If a filename contains no pneumonia it contains a single row with an empty pneumonia location.\n",
    "\n",
    "The code below loads the table and transforms it into a dictionary. \n",
    "* The dictionary uses the filename as key and a list of pneumonia locations in that filename as value. \n",
    "* If a filename is not present in the dictionary it means that it contains no pneumonia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4bfa4e256e71d8b3ef332a4ad6e7208018f0d8b7"
   },
   "outputs": [],
   "source": [
    "# empty dictionary\n",
    "pneumonia_locations = {}\n",
    "# load table\n",
    "with open(os.path.join('../input/stage_1_train_labels.csv'), mode='r') as infile:\n",
    "    # open reader\n",
    "    reader = csv.reader(infile)\n",
    "    # skip header\n",
    "    next(reader, None)\n",
    "    # loop through rows\n",
    "    for rows in reader:\n",
    "        # retrieve information\n",
    "        filename = rows[0]\n",
    "        location = rows[1:5]\n",
    "        pneumonia = rows[5]\n",
    "        # if row contains pneumonia add label to dictionary\n",
    "        # which contains a list of pneumonia locations per filename\n",
    "        if pneumonia == '1':\n",
    "            # convert string to float to int\n",
    "            location = [int(float(i)) for i in location]\n",
    "            # save pneumonia location in dictionary\n",
    "            if filename in pneumonia_locations:\n",
    "                pneumonia_locations[filename].append(location)\n",
    "            else:\n",
    "                pneumonia_locations[filename] = [location]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "32307ad2d6f0f77bbb1b99bfd8b79dc9d8cb2f8b"
   },
   "source": [
    "## Data Preparation 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6fb5fcf200c6621ec3f727e5afdcf9e313318db4"
   },
   "source": [
    "### Data generator\n",
    "\n",
    "The dataset is too large to fit into memory, so we need to create a generator that loads data on the fly.\n",
    "\n",
    "* The generator takes in some filenames, batch_size and other parameters.\n",
    "\n",
    "* The generator outputs a random batch of numpy images and numpy masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6ce31b844747120578a477a4e65d1dd4ae3766bc"
   },
   "outputs": [],
   "source": [
    "class generator(keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self, folder, filenames, pneumonia_locations=None, batch_size=32, image_size=256, shuffle=True, augment=False, predict=False):\n",
    "        self.folder = folder\n",
    "        self.filenames = filenames\n",
    "        self.pneumonia_locations = pneumonia_locations\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.shuffle = shuffle\n",
    "        self.augment = augment\n",
    "        self.predict = predict\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __load__(self, filename):\n",
    "        # load dicom file as numpy array\n",
    "        img = pydicom.dcmread(os.path.join(self.folder, filename)).pixel_array\n",
    "        # create empty mask\n",
    "        msk = np.zeros(img.shape)\n",
    "        # get filename without extension\n",
    "        filename = filename.split('.')[0]\n",
    "        # if image contains pneumonia\n",
    "        if filename in self.pneumonia_locations:\n",
    "            # loop through pneumonia\n",
    "            for location in self.pneumonia_locations[filename]:\n",
    "                # add 1's at the location of the pneumonia\n",
    "                x, y, w, h = location\n",
    "                msk[y:y+h, x:x+w] = 1\n",
    "        # resize both image and mask\n",
    "        img = resize(img, (self.image_size, self.image_size), mode='reflect')\n",
    "        msk = resize(msk, (self.image_size, self.image_size), mode='reflect') > 0.5\n",
    "        # if augment then horizontal flip half the time\n",
    "        if self.augment and random.random() > 0.5:\n",
    "            img = np.fliplr(img)\n",
    "            msk = np.fliplr(msk)\n",
    "        # add trailing channel dimension\n",
    "        img = np.expand_dims(img, -1)\n",
    "        msk = np.expand_dims(msk, -1)\n",
    "        return img, msk\n",
    "    \n",
    "    def __loadpredict__(self, filename):\n",
    "        # load dicom file as numpy array\n",
    "        img = pydicom.dcmread(os.path.join(self.folder, filename)).pixel_array\n",
    "        # resize image\n",
    "        img = resize(img, (self.image_size, self.image_size), mode='reflect')\n",
    "        # add trailing channel dimension\n",
    "        img = np.expand_dims(img, -1)\n",
    "        return img\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # select batch\n",
    "        filenames = self.filenames[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        # predict mode: return images and filenames\n",
    "        if self.predict:\n",
    "            # load files\n",
    "            imgs = [self.__loadpredict__(filename) for filename in filenames]\n",
    "            # create numpy batch\n",
    "            imgs = np.array(imgs)\n",
    "            return imgs, filenames\n",
    "        # train mode: return images and masks\n",
    "        else:\n",
    "            # load files\n",
    "            items = [self.__load__(filename) for filename in filenames]\n",
    "            # unzip images and masks\n",
    "            imgs, msks = zip(*items)\n",
    "            # create numpy batch\n",
    "            imgs = np.array(imgs)\n",
    "            msks = np.array(msks)\n",
    "            return imgs, msks\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            random.shuffle(self.filenames)\n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.predict:\n",
    "            # return everything\n",
    "            return int(np.ceil(len(self.filenames) / self.batch_size))\n",
    "        else:\n",
    "            # return full batches only\n",
    "            return int(len(self.filenames) / self.batch_size)\n",
    "        \n",
    "def data_func1():\n",
    "    #load data param\n",
    "    n_valid_samples = data_param_list[data_func_idx][data_param_idx]\n",
    "    print('Number of Validation Samples [%d]'%(n_valid_samples))\n",
    "    \n",
    "    # create train and validation generators\n",
    "    # load and shuffle filenames\n",
    "    folder = '../input/stage_1_train_images'\n",
    "    filenames = os.listdir(folder)\n",
    "    random.shuffle(filenames)\n",
    "    # split into train and validation filenames\n",
    "    train_filenames = filenames[n_valid_samples:]\n",
    "    valid_filenames = filenames[:n_valid_samples]\n",
    "    print('n train samples', len(train_filenames))\n",
    "    print('n valid samples', len(valid_filenames))\n",
    "    n_train_samples = len(filenames) - n_valid_samples\n",
    "    \n",
    "    folder = '../input/stage_1_train_images'\n",
    "    train_gen = generator(folder, train_filenames, pneumonia_locations, batch_size=32, image_size=256, shuffle=True, augment=True, predict=False)\n",
    "    valid_gen = generator(folder, valid_filenames, pneumonia_locations, batch_size=32, image_size=256, shuffle=False, predict=False)\n",
    "    return (train_gen, valid_gen)\n",
    "\n",
    "#add data prep function and paramters \n",
    "i = len(data_func_list)                #find idx of next entry\n",
    "data_func_list.append(data_func1)     #append the function pointer to the list, replace with your function name\n",
    "data_param_list.append([])             #keep this line even if there's no param\n",
    "data_param_list[i].append(2560)        #keep this line even if there's no param\n",
    "data_param_list[i].append(5120)        #keep this line even if there's no param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e67183b8eebb9beae9b7060be7870e9dea8809b0"
   },
   "source": [
    "## Data Preparation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "04d73b125b4c9520c91c7bcd6ce2bbdf10b38043"
   },
   "outputs": [],
   "source": [
    "# put code here\n",
    "\n",
    "#add data prep function and paramters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ebc622a4b406354cc4ef28801eab72346a724d8b"
   },
   "source": [
    "# Build Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0052d52984148e0a5c5798b4c131fd2fc970d295"
   },
   "source": [
    "## Model 1 - RESNET5 CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9fc2b108689637a6037b48ebab3f7659b8704bf9"
   },
   "outputs": [],
   "source": [
    "def create_downsample(channels, inputs):\n",
    "    x = keras.layers.BatchNormalization(momentum=0.9)(inputs)\n",
    "    x = keras.layers.LeakyReLU(0)(x)\n",
    "    x = keras.layers.Conv2D(channels, 1, padding='same', use_bias=False)(x)\n",
    "    x = keras.layers.MaxPool2D(2)(x)\n",
    "    return x\n",
    "\n",
    "def create_resblock(channels, inputs):\n",
    "    x = keras.layers.BatchNormalization(momentum=0.9)(inputs)\n",
    "    x = keras.layers.LeakyReLU(0)(x)\n",
    "    x = keras.layers.Conv2D(channels, 3, padding='same', use_bias=False)(x)\n",
    "    x = keras.layers.BatchNormalization(momentum=0.9)(x)\n",
    "    x = keras.layers.LeakyReLU(0)(x)\n",
    "    x = keras.layers.Conv2D(channels, 3, padding='same', use_bias=False)(x)\n",
    "    return keras.layers.add([x, inputs])\n",
    "\n",
    "def create_network(input_size, channels, n_blocks=2, depth=4):\n",
    "    # input\n",
    "    inputs = keras.Input(shape=(input_size, input_size, 1))\n",
    "    x = keras.layers.Conv2D(channels, 3, padding='same', use_bias=False)(inputs)\n",
    "    # residual blocks\n",
    "    for d in range(depth):\n",
    "        channels = channels * 2\n",
    "        x = create_downsample(channels, x)\n",
    "        for b in range(n_blocks):\n",
    "            x = create_resblock(channels, x)\n",
    "    # output\n",
    "    x = keras.layers.BatchNormalization(momentum=0.9)(x)\n",
    "    x = keras.layers.LeakyReLU(0)(x)\n",
    "    x = keras.layers.Conv2D(1, 1, activation='sigmoid')(x)\n",
    "    outputs = keras.layers.UpSampling2D(2**depth)(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "def model_function1(input_size):\n",
    "    #load model_param\n",
    "    (channels,n_blocks,depth) = model_param_list[model_func_idx][model_param_idx]\n",
    "    print('model_function1 RESNET-5: channels[%d], n_blocks[%d], depth[%d]'% (channels,n_blocks,depth))\n",
    "    return create_network(input_size, channels, n_blocks, depth)\n",
    "\n",
    "#add model and paramters \n",
    "i = len(model_func_list)                #find idx of next entry\n",
    "model_func_list.append(model_function1)     #append the function pointer to the list, replace with your function name\n",
    "model_param_list.append([])             #keep this line even if there's no param\n",
    "model_param_list[i].append((32,2,4))       #append as many parameter tuples as you want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "924bf37aa0253b647d78c9fb4b6e75973b4c4c9b"
   },
   "source": [
    "## Model 2 - UNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "74c46333dc477e6b7ae67a2ef344ee62c9743d30"
   },
   "outputs": [],
   "source": [
    "#put code here\n",
    "def build_UNet2D_4L(inp_shape, k_size=3):\n",
    "    merge_axis = -1 # Feature maps are concatenated along last axis (for tf backend)\n",
    "    data = keras.Input(shape=inp_shape)\n",
    "    conv1 = keras.layers.Convolution2D(filters=32, kernel_size=k_size, padding='same', activation='relu')(data)\n",
    "    conv1 = keras.layers.Convolution2D(filters=32, kernel_size=k_size, padding='same', activation='relu')(conv1)\n",
    "    pool1 = keras.layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = keras.layers.Convolution2D(filters=64, kernel_size=k_size, padding='same', activation='relu')(pool1)\n",
    "    conv2 = keras.layers.Convolution2D(filters=64, kernel_size=k_size, padding='same', activation='relu')(conv2)\n",
    "    pool2 = keras.layers.MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = keras.layers.Convolution2D(filters=64, kernel_size=k_size, padding='same', activation='relu')(pool2)\n",
    "    conv3 = keras.layers.Convolution2D(filters=64, kernel_size=k_size, padding='same', activation='relu')(conv3)\n",
    "    pool3 = keras.layers.MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = keras.layers.Convolution2D(filters=128, kernel_size=k_size, padding='same', activation='relu')(pool3)\n",
    "    conv4 = keras.layers.Convolution2D(filters=128, kernel_size=k_size, padding='same', activation='relu')(conv4)\n",
    "    pool4 = keras.layers.MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    conv5 = keras.layers.Convolution2D(filters=256, kernel_size=k_size, padding='same', activation='relu')(pool4)\n",
    "\n",
    "    up1 = keras.layers.UpSampling2D(size=(2, 2))(conv5)\n",
    "    conv6 = keras.layers.Convolution2D(filters=256, kernel_size=k_size, padding='same', activation='relu')(up1)\n",
    "    conv6 = keras.layers.Convolution2D(filters=256, kernel_size=k_size, padding='same', activation='relu')(conv6)\n",
    "    merged1 = keras.layers.concatenate([conv4, conv6], axis=merge_axis)\n",
    "    conv6 = keras.layers.Convolution2D(filters=256, kernel_size=k_size, padding='same', activation='relu')(merged1)\n",
    "\n",
    "    up2 = keras.layers.UpSampling2D(size=(2, 2))(conv6)\n",
    "    conv7 = keras.layers.Convolution2D(filters=256, kernel_size=k_size, padding='same', activation='relu')(up2)\n",
    "    conv7 = keras.layers.Convolution2D(filters=256, kernel_size=k_size, padding='same', activation='relu')(conv7)\n",
    "    merged2 = keras.layers.concatenate([conv3, conv7], axis=merge_axis)\n",
    "    conv7 = keras.layers.Convolution2D(filters=256, kernel_size=k_size, padding='same', activation='relu')(merged2)\n",
    "\n",
    "    up3 = keras.layers.UpSampling2D(size=(2, 2))(conv7)\n",
    "    conv8 = keras.layers.Convolution2D(filters=128, kernel_size=k_size, padding='same', activation='relu')(up3)\n",
    "    conv8 = keras.layers.Convolution2D(filters=128, kernel_size=k_size, padding='same', activation='relu')(conv8)\n",
    "    merged3 = keras.layers.concatenate([conv2, conv8], axis=merge_axis)\n",
    "    conv8 = keras.layers.Convolution2D(filters=128, kernel_size=k_size, padding='same', activation='relu')(merged3)\n",
    "\n",
    "    up4 = keras.layers.UpSampling2D(size=(2, 2))(conv8)\n",
    "    conv9 = keras.layers.Convolution2D(filters=64, kernel_size=k_size, padding='same', activation='relu')(up4)\n",
    "    conv9 = keras.layers.Convolution2D(filters=64, kernel_size=k_size, padding='same', activation='relu')(conv9)\n",
    "    merged4 = keras.layers.concatenate([conv1, conv9], axis=merge_axis)\n",
    "    conv9 = keras.layers.Convolution2D(filters=64, kernel_size=k_size, padding='same', activation='relu')(merged4)\n",
    "\n",
    "    conv10 = keras.layers.Convolution2D(filters=1, kernel_size=k_size, padding='same', activation='sigmoid')(conv9)\n",
    "\n",
    "    output = conv10\n",
    "    model = keras.Model(data, output)\n",
    "    return model\n",
    "\n",
    "def model_function2(input_size):\n",
    "    #load model_param\n",
    "    k_size = model_param_list[model_func_idx][model_param_idx]\n",
    "    print('model_function2 UNET: ksize[%d]'% (k_size))\n",
    "    \n",
    "    inp_shape = (input_size,input_size, 1)\n",
    "    return build_UNet2D_4L(inp_shape, k_size)\n",
    "\n",
    "#add model and paramters \n",
    "i = len(model_func_list)                #find idx of next entry\n",
    "model_func_list.append(model_function2)     #append the function pointer to the list, replace with your function name\n",
    "model_param_list.append([])             #keep this line even if there's no param\n",
    "model_param_list[i].append((3))       #append as many parameter tuples as you want\n",
    "#add model and paramters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a26f2c37e753fa70e02b05440c4d74a2d989d409"
   },
   "source": [
    "## Model 3- TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f0ec2d2c3d6aca71100cdd7fa276f1486aa6ce3e"
   },
   "outputs": [],
   "source": [
    "#put code here\n",
    "\n",
    "#add model and paramters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bee3bc9363e9c1829eadf17da7a67fbd1a6a369e"
   },
   "source": [
    "# Build Loss Function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c5efb02b0eb604b360e9476a5dc50a12bac5a078"
   },
   "source": [
    "## Loss function 1 - IOU + BCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4369be30f61440eb6858d57829fa541c4ee893bf"
   },
   "outputs": [],
   "source": [
    "# define iou or jaccard loss function\n",
    "def iou_loss(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, [-1])\n",
    "    y_pred = tf.reshape(y_pred, [-1])\n",
    "    intersection = tf.reduce_sum(y_true * y_pred)\n",
    "    score = (intersection + 1.) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection + 1.)\n",
    "    return 1 - score\n",
    "\n",
    "# combine bce loss and iou loss\n",
    "def iou_bce_loss(y_true, y_pred):\n",
    "     #load model_param\n",
    "    (bce_weight,iou_weight) = loss_param_list[loss_func_idx][loss_param_idx]\n",
    "    print('iou_bce_loss: bce_weight[%f], iou_weight[%f]'% (bce_weight,iou_weight))\n",
    "    return bce_weight * keras.losses.binary_crossentropy(y_true, y_pred) + iou_weight * iou_loss(y_true, y_pred)\n",
    "\n",
    "#add loss function and paramters \n",
    "i = len(loss_func_list)                #find idx of next entry\n",
    "loss_func_list.append(iou_bce_loss)      #append the function pointer to the list, replace with your function name\n",
    "loss_param_list.append([])             #keep this line even if there's no param\n",
    "loss_param_list[i].append((0.5,0.5))   #append as many parameter tuples as you want\n",
    "loss_param_list[i].append((0.6,0.4))\n",
    "loss_param_list[i].append((0.4,0.6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "669f87a84e846510106b173aabc39dc86036a890"
   },
   "source": [
    "# Complete Abstract Layer\n",
    "Assign selected function pointers to common function pointer to be used in following code in main body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fdda910ba11fc37e1f99333902cd3579cb449997"
   },
   "outputs": [],
   "source": [
    "#Assign selected function pointer to main function pointer\n",
    "data_func = data_func_list[data_func_idx]\n",
    "model_func = model_func_list[model_func_idx]\n",
    "loss_func = loss_func_list[loss_func_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2600821d4dd7dc1b7fd0baeb0a3fdcc708f61e42"
   },
   "source": [
    "# Prepare Additional Static Training Factors\n",
    "For the ones not requiring tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "08185433f538dfb2beebd47c7b53b308eec860be"
   },
   "outputs": [],
   "source": [
    "# mean iou as a metric\n",
    "def mean_iou(y_true, y_pred):\n",
    "    y_pred = tf.round(y_pred)\n",
    "    intersect = tf.reduce_sum(y_true * y_pred, axis=[1, 2, 3])\n",
    "    union = tf.reduce_sum(y_true, axis=[1, 2, 3]) + tf.reduce_sum(y_pred, axis=[1, 2, 3])\n",
    "    smooth = tf.ones(tf.shape(intersect))\n",
    "    return tf.reduce_mean((intersect + smooth) / (union - intersect + smooth))\n",
    "\n",
    "# cosine learning rate annealing\n",
    "def cosine_annealing(x):\n",
    "    lr = 0.001\n",
    "    epochs = EPOCHS\n",
    "    return lr*(np.cos(np.pi*x/epochs)+1.)/2\n",
    "learning_rate = tf.keras.callbacks.LearningRateScheduler(cosine_annealing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "982497faef3f7d7a270746456a2b0f33656c6923"
   },
   "source": [
    "# Training\n",
    "Use abstracted functions in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b23bda8b26bfc4bf7fff52c780c9b11f87f4d513"
   },
   "outputs": [],
   "source": [
    "# create network and compiler\n",
    "model = model_func(input_size=256)\n",
    "model.compile(optimizer='adam',\n",
    "              loss=loss_func,\n",
    "              metrics=['accuracy', mean_iou])\n",
    "\n",
    "# create data set\n",
    "(train_gen, valid_gen) = data_func()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a6ed6992bc223f23ccaba6fac331fbed5d6ca4cf"
   },
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a73bdd2fbbbb3a0bca1e9dc1b0541dd1d9e3ab0c"
   },
   "outputs": [],
   "source": [
    "history = model.fit_generator(train_gen, validation_data=valid_gen, callbacks=[learning_rate], epochs=EPOCHS, workers=4, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a735ee2a1427640aadf94b0429a006213c925700"
   },
   "source": [
    "## Review History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3666ba4cac9ed2c3029b824af220404bfcc16f23"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(131)\n",
    "plt.plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\n",
    "plt.plot(history.epoch, history.history[\"val_loss\"], label=\"Valid loss\")\n",
    "plt.legend()\n",
    "plt.subplot(132)\n",
    "plt.plot(history.epoch, history.history[\"acc\"], label=\"Train accuracy\")\n",
    "plt.plot(history.epoch, history.history[\"val_acc\"], label=\"Valid accuracy\")\n",
    "plt.legend()\n",
    "plt.subplot(133)\n",
    "plt.plot(history.epoch, history.history[\"mean_iou\"], label=\"Train iou\")\n",
    "plt.plot(history.epoch, history.history[\"val_mean_iou\"], label=\"Valid iou\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c2dedeede333f5d8651fc386b1d959f3bd8964ac"
   },
   "source": [
    "## Validate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fbbf7546d396560d21b7af17d4c959713f425d8e"
   },
   "outputs": [],
   "source": [
    "for imgs, msks in valid_gen:\n",
    "    # predict batch of images\n",
    "    preds = model.predict(imgs)\n",
    "    # create figure\n",
    "    f, axarr = plt.subplots(4, 8, figsize=(20,15))\n",
    "    axarr = axarr.ravel()\n",
    "    axidx = 0\n",
    "    # loop through batch\n",
    "    for img, msk, pred in zip(imgs, msks, preds):\n",
    "        # plot image\n",
    "        axarr[axidx].imshow(img[:, :, 0])\n",
    "        # threshold true mask\n",
    "        comp = msk[:, :, 0] > 0.5\n",
    "        # apply connected components\n",
    "        comp = measure.label(comp)\n",
    "        # apply bounding boxes\n",
    "        predictionString = ''\n",
    "        for region in measure.regionprops(comp):\n",
    "            # retrieve x, y, height and width\n",
    "            y, x, y2, x2 = region.bbox\n",
    "            height = y2 - y\n",
    "            width = x2 - x\n",
    "            axarr[axidx].add_patch(patches.Rectangle((x,y),width,height,linewidth=2,edgecolor='b',facecolor='none'))\n",
    "        # threshold predicted mask\n",
    "        comp = pred[:, :, 0] > 0.5\n",
    "        # apply connected components\n",
    "        comp = measure.label(comp)\n",
    "        # apply bounding boxes\n",
    "        predictionString = ''\n",
    "        for region in measure.regionprops(comp):\n",
    "            # retrieve x, y, height and width\n",
    "            y, x, y2, x2 = region.bbox\n",
    "            height = y2 - y\n",
    "            width = x2 - x\n",
    "            axarr[axidx].add_patch(patches.Rectangle((x,y),width,height,linewidth=2,edgecolor='r',facecolor='none'))\n",
    "        axidx += 1\n",
    "    plt.show()\n",
    "    # only plot one batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "25dc7ad848190bb37349a80f96ed2bdb5c3821b0"
   },
   "source": [
    "# Predict test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2c9277e4ec9f12712dd690002c540b396278c504",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load and shuffle filenames\n",
    "folder = '../input/stage_1_test_images'\n",
    "test_filenames = os.listdir(folder)\n",
    "print('n test samples:', len(test_filenames))\n",
    "\n",
    "# create test generator with predict flag set to True\n",
    "test_gen = generator(folder, test_filenames, None, batch_size=25, image_size=256, shuffle=False, predict=True)\n",
    "\n",
    "# create submission dictionary\n",
    "submission_dict = {}\n",
    "# loop through testset\n",
    "for imgs, filenames in test_gen:\n",
    "    # predict batch of images\n",
    "    preds = model.predict(imgs)\n",
    "    # loop through batch\n",
    "    for pred, filename in zip(preds, filenames):\n",
    "        # resize predicted mask\n",
    "        pred = resize(pred, (1024, 1024), mode='reflect')\n",
    "        # threshold predicted mask\n",
    "        comp = pred[:, :, 0] > 0.5\n",
    "        # apply connected components\n",
    "        comp = measure.label(comp)\n",
    "        # apply bounding boxes\n",
    "        predictionString = ''\n",
    "        for region in measure.regionprops(comp):\n",
    "            # retrieve x, y, height and width\n",
    "            y, x, y2, x2 = region.bbox\n",
    "            height = y2 - y\n",
    "            width = x2 - x\n",
    "            # proxy for confidence score\n",
    "            conf = np.mean(pred[y:y+height, x:x+width])\n",
    "            # add to predictionString\n",
    "            predictionString += str(conf) + ' ' + str(x) + ' ' + str(y) + ' ' + str(width) + ' ' + str(height) + ' '\n",
    "        # add filename and predictionString to dictionary\n",
    "        filename = filename.split('.')[0]\n",
    "        submission_dict[filename] = predictionString\n",
    "    # stop if we've got them all\n",
    "    if len(submission_dict) >= len(test_filenames):\n",
    "        break\n",
    "\n",
    "# save dictionary as csv file\n",
    "sub = pd.DataFrame.from_dict(submission_dict,orient='index')\n",
    "sub.index.names = ['patientId']\n",
    "sub.columns = ['PredictionString']\n",
    "sub.to_csv('submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
